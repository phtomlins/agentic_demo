I want to build job specific CVs. But, rather than give you a CV I am going to give you my reflections no my career and ask you to turn this into a CV.

I'll start at the beginning pre-university to give you context. You decide what goes on the CV and how to tailor it.

You are my job search advisor and consultant. You need to critique everything. You need to help me communicate my skills. I am not looking for affirmation that makes me feel good, I am looking to you as my consultant to get me interviews and help prepare for them. I have not heard back from any of the CVs that I have sent so far - but they were produced by giving you a CV and asking you to tailor it to a role.

Here goes:
I originally did not go to university. I was really interested in computers and IT and got a IT support technician job in my local sixth form college – the one at which I had studied for my A-levels. As a kid I used to tinker with old computers (circa 1995-1998), networking them, experimenting with differenting operating systems (MS DOS, Windows 3.1 – 98/NT 4 etc etc, Linux, IBM OS2) and I taught myself to code in QBasic using PC Plus magazine. I started building PCs, ordering parts from parts magazines too. 

The college IT team of two had left, so the team was being rebuilt from scratch. Myself and a former student both got the junior IT support technician roles and one of the chemistry teachers picked up the job of head of IT. He had a PhD in chemistry.

My job spanned desktop user support in addition to installing new networks, building PCs (I built hundreds!) supporting Novell Netware 4, installing and setting up Redhat Linux servers and Windows NT servers. It was a highly collaborative role, mainly user facing, with a lot of troubleshooting. I started this role in September 1998 and did this full-time for a year until my boss convinced me that I should go to university. I decided to go to study Physics at Lancaster University, UK, starting in September 1999.

My original plan at Lancaster was to undertake a 3 year BSc Physics with the 2nd year in the USA. However, during the initial part of my first year I followed poor advice from my friends and did not study or work hard – apparently the first year doesn’t count. I lost sight of needing a top grade in order to qualify for the 2nd year in the USA. In fact, my director of studies called me in one day to inform me that if I did not pass all remaining exams with a first class grade (in the UK that is >70% - which is very challenging to achieve, or it was in 1999-2002), then I would not be admitted back to the university for my second year at all. The USA was out of the question regardless. Disappointed, I set my sights on passing the year. I discovered that by applying myself to study I could excel. I also enjoyed the subject matter. So, this led to me being awarded the Physics Prize in my second and 3rds years – this is the award for achieving the highest grade in the year. I passed my Physics degree at the top of the class with 1st Class honours, graduating in July 2002. 

During my degree I specialised in computational Physics, which meant studying all of the core Physics modules, but undertaking computational simulation projects. I implemented a numerical study of Rutherford’s scattering experiment, and I explored the impact of relativity on the orbital motion of planetary bodies, comparing with the predictions of Newtonian mechanics. In a third project I solved the wave equations to show the expected magnitude of gravitational wave interaction with an experiment – at that time gravitational waves were yet to be confirmed experimentally. These projects were implemented in Fortran 95 and Maple. This gave me a grounding in the basics of numerical methods and solving differential equations numerically.

Whilst studying for my finals I applied for several graduate jobs. I did not get many interviews, I have always assumed this is because my A-level results were not very impressive (B – Physics, C – Computing, C – Maths, failed AS Further maths: all a consequence of no structure to my self directed study and lack of commitment as a 16-18 yr old). Although, more recently I have wondered whether it has more to do with my limited communications skills and being unable to concisely and impressively communicate my suitability for a given role in a way that resonates immediately with the hiring manager. 

During this time, I did have a couple of interviews that set the tone (and my insecurity) for many technical interviews since: Namely, I felt humiliated by being asked technical questions, some mathematical, some practical computer coding. Every time I fell flat, not seeing the thing that the interviewer was driving at and appearing technically incompetent. In one example, I was interviewing for a job with a satellite technology company. The interview covered my project work on orbital motion, I could not solve the basic equations that I had been working with daily on my project. I would have not seemed very bright to the interviewer. However, I am confident that having been in the role I would have excelled. These experiences cemented what has now become entrenched as my own insecurity that I do not perform well when being tested. But, whenever in a role I always excel beyond my peers. In this example, I achieved the top project grade – yet could not demonstrate my ability in interview. My interview with the National Physical Laboratory appeared to me to follow the same course. I was asked to outline the order of magnitude of wavelengths across the electromagnetic spectrum. I remember saying “I don’t know, this is embarrassing for a 1st class physics candidate”, but then I went on to derive it all based on a couple of pieces of knowledge, 1) the concept of red-shift means the red must have a longer wavelength than blue, so by extension infra-red must be longer than red. 2) My clock radio antenna was about a meter long, so radio waves must be in the meter range. They also asked me to design a cryogenic radiometer with items laying around the house – I don’t remember how I answered, but I feel like I fumbled through. I was offered a job in the centre for electromagnetic and time metrology. So, I assume that despite my lack of memory, by use of physical principles to derive the EM spectrum impressed them enough.

I joined NPL as a research scientist in September 2002 as a graduate member of the Photonics group. The photonics group primarily existed to calibrate optical telecoms equipment, i.e. power meter calibration, return loss of optical fibres, polarization characterisation, wavelength calibration. Each measurement service was valuable to large telecoms clients, such as Bookham, because they offered traceability to National Standards and came with a full uncertainty budget which was critical to their service offering. I undertook courses in the mathematical principles of measurement uncertainty and developed a core understanding of its statistical principles and how this related to what is and is not measurable in the real world. Honestly, I found it a bit boring and a bit disappointing. I wanted to be working cutting edge research, but was working in a cash strapped calibration lab. For a time I delivered the fibre optic power meter calibration service. At the time I found it dull. I now look back and see the value in the training. Especially in understanding the control necessary to achieve reproducible experimental results – environmental factor basically change everything. In the real world it’s unlikely that any physical measurement system will perform as well as in a calibration lab.

I shared an office with a talented physicist who was excellent at both theoretical and computational physics and the experimental work. From him I adopted MathCAD as a tool for building physics based models of experimental systems and predicting their performance. These models were based on physical laws and not statistical. I found that my real interested was in the computation modelling and the analysis of experimental results.

I was fortunate enough to work on several interesting projects to investigate the characterisation of non-linear effects in fibre optic telecoms systems (double Raman scattering, Raman amplifiers, Four Wave Mixing – implementing simulations in MathCAD and VB.Net). Upon reflection, my actual contribution to these fields was nil, but I learned a lot, working largely independently to build the mathematical and computational tools. It was here that I encountered the NAG libraries (numerical algorithm group), and gained appreciation for validated, numerically stable algorithms. I also had a fundamental physicist need to understand how things work, so I implemented a lot of algorithms independently in MathCAD – I remember implementing Singular Value Decompostion and the Levenberg Marquardt optimisation algorithm. I also encountered S+ and R for the first time, I had to implement some Fortran library interface to R, the Prony method for decomposition of periodic functions. I also started developing a keen understanding of Fourier analysis and Fourier transforms which become foundational for all of my future work. I worked somewhat alone on the non-linear fibre optics projects, reaching out to colleagues to help with some of the maths. I also worked on a project to turn a reference refractometer system into a system for accurate measurement of surface plasmon resonance. The idea was to work alongside the biotech group to make precise measurements of the surface binding properties of proteins – from memory this streptavidin – although this was circa 2003, so things are hazy. The idea was to do a PhD modelling the physics of protein binding on surfaces and develop the reference refractometer to be able to make corresponding measurements. This did not go anywhere – but is piqued my interest for both the measurement of biology and refractive index as an important measurand. The foundation was laid. My group leader asked me to write a report exploring the opportunities for photonics technology in biology and medicine. This was in 2003 when the telecoms bubble had burst and the money funding my position was running out.  As part of this work, I attended a conference at Cranfield University on optical coherence tomography (OCT). OCT was interesting because it used telecoms components (optical fibres, splitted, detectors) that I was both familiar with and had access to. It also is based upon interferometry, a technology well known to the metrology world. OCT produces images of tissue and materials based upon its light scattering characteristics, a modulation in the 3D refractive index. I was hooked. At NPL, the emphasis was always on measurement. I had found a technology that I could sell to the lab as a measurement tool and in need of quantitative, traceable measurement science. It related to the refractive index work, and had a clear biomedical application. I enrolled on a PhD at Cranfield University with Prof Ricky Wang. My line manager had advised that she thought he would be a good supervisor. I quite liked his energy and the speed at which he worked. He just got on with getting things done. My PhD focussed on measuring the scattering properties of ‘biological material’, although really I was interested in estimating refractive index variations. One application was dental composite curing, which I was able to monitor dynamically using my OCT setup – I did this in collaboration with Birmingham University through an introduction to a post-doc, just a few years ahead of me. During my PhD I built a bespoke OCT system (including software written in C# - the software itself was functional – but a design and maintainability nightmare, being typical academic/scientific spaghetti code. But, I shipped and supported this myself for many years. It kind of cemented my ‘get things done’ attitude in terms of delivering a product) for Birmingham University. This system measured the refractive index of curing dental composites and enabled them to publish many papers on the subject. It used a core innovation of mine which was to measure at multiple angles and then determine average refractive index from the minimum of the optical path lengths. I published the basic technique in optics letters. I became fascinated by the inverse problem of determining refractive index properties by measuring backscatter from multiple angles. My PhD study concluded before I matured these ideas. My supervisor encouraged me to submit my thesis within about 2.5 years. A PhD is research training, not a life’s work. The ideas generated within my DPhil continue to influence my work even to this day. I say my PhD is in Physics because the work was all optical physics. All of the experimental work was undertaken at the National Physical Laboratory who paid for it. However, I was for this time enrolled at Cranfield University (April 2004- Graduated June 2008). The title was The Measurement of Biological Material Using Optical Coherence Tomography.

During my PhD, Ricky moved from Cranfield University to Oregon Health & Science University, Portland, USA. I spent Jan – March 2006 as a visiting scientist at OHSU working in Ricky’s lab alongside his embroyonic team. It was at that time I developed the techniques used for measuring dental composite and glucose solutions – this led to my interest in how refractive index can measure protein concentrations. The technical work and analysis was largely conducted in MathCAD.

At NPL, I was the inaugural member of the biophotonics group. I won a large multi-million £ grant to fund my PhD. As a result I was asked to recruit additional people to contribute to the project. So, I organically build NPL’s biophotonics group from just me to 7 people from 2004 until I left in July 2010.  I was promoted to higher research scientist, then senior research scientist and my leadership of the group was formally recognised by my appointment as Technical Area Leader for Biophotonics. I don’t remember the dates now. I remained technically hands-on throughout, but coordinated across delivery of multiple biophotonics projects, primarily ensuring that our group collaborated with others in NPL, notably the biotechnology group. I played a key role in writing the grant applications and developing relationships with industrial partners. Across my 8 years at NPL my projects and income were worth over £5m. I built the biophotonics group to 7 people + collaborators, and operated two NPL optics laboratories dedicated to biophotonics research, mostly adaptive optics and OCT. 

Just prior to me leaving, this success was recognised by the merging of the biophotonics and biotechnology groups. To this day, the merged group continues to thrive and builds upon super-resolution microscopy techniques that I introduced just prior to leaving in 2010. In fact, NPL has a multi-million pound business now built on the biotechnology group’s work that traces it’s routes to the foundations that I built with biophotonics and pre-PhD on SPR – my boss at that time was married to the founder of the initial biotechnology group – so this all happened in parallel.

Working at a National metrology lab, I developed a keen interest in standardisation and reference measurements. For OCT I led a programme to develop calibration phantoms and standards for OCT. I was particularly interested in characterising the point-spread function of OCT imaging systems and calibrated scattering phantoms. My team developed methods for creating such phantoms using iron Oxide emebedded in polyurethane and also using femto-second laser inscription, the latter was a collaboration with Aston University and co-supervision of a PhD student there. I also picked up the thesis of former Oregon Health and Science University researcher, Scott Prahl, who had developed monte carlo simulation software to predict light propagation in scattering media. I modified his C code (I am not a C developer – I have used it out of necessity from time to time, but retain no memory of it!) to log the optical path length so that I can use this as a reference for phantom based measurements. I did publish a talk at Photonics West, but never got this into a peer reviewed journal paper. Nevertheless, this kicked off my interest in Monte Carlo methods for statistical simulation, which has become important throughout my career. The OCT phantoms project attracted interest from FDA researchers, with whom I collaborated and co-published journal articles. General Purpose GPUs from NVIDIA were just becoming mainstream, and my group became early adopters using these mainly for their highly parallel computation of Fourier transforms for OCT processing in real-time (also published with my Aston Uni PhD student).

As a side note – whilst at NPL, my interest in mathematical modelling and data led me to explore paper on neural networks. I implemented some of the training methods such as conjugate gradient descent and built simple experiments in MathCAD of basic neural network architectures. I was doing this circa 2008, with no idea how ML and AI would explode. I had a side interest in time-series analysis and making my fortune by automated stock trading – sadly never realised, but these interests are foundational to my career.

During this time, I realised that working in a physics lab was too far from the biomedical application. So, I started formal collaboration with two clinical groups. One was the biophotonics group based at Gloucestershire Royal Hospital, run by Nicholas Stone and funded largely by various grants (won by Nick, I had nothing to do with their funding). They specialised in Raman spectroscopy, but had some OCT activity and we had a common link back to my PhD supervisor. I hosted their staff in my lab at NPL and my staff spent time at the GRH Biophotonics Research Group. Eventually I arranged for one of my team to be embedded at the GRU on a permanent basis, establishing an NPL satellite centre on a clinical site. Simultaneously, I setup a collaboration with Barts and the London School of Medicine and Dentistry (Queen Mary University of London). Professor Farida Fortune wanted to use OCT to measure and monitor oral epithelia dysplasia. My PhD work aligned with what she wanted to do, so I began working with one of her PhD students and published on the subject. This collaboration was key. An opportunity for a senior lectureship arose within the institute of dentistry at QMUL. Farida was Dean of the institute of dentistry at the time and invited me to apply. I applied and was appointed as a senior lecturer, starting in August 2010. The branding of the role is a little confusing. I tend to tailor this to whatever I am applying for. Technically, I was a Senior Lecturer in Dental Biometrics at Queen Mary University of London, Barts and The London School of Medicine & Dentistry, Institute of Dentistry. However, sometimes I just use the Barts and the London School of Medicine and Dentistry – it’s the most well known brand (I think) and covers the breadth of my role across medicine and dentistry. For the purposes of this narrative, I will just use QMUL to refer to this role.

So, what did I do at QMUL? I started working in Farida’s group in oral medicine. Farida is qualified in both medicine and dentistry. She is a specialist in B	echet’s disease – although I never got into this. I collaborated within a small group, 2-5 interdisciplinary scientists and clinicians (physics, biology, medicine, dentistry, pathology). I built a non-clinical/portable OCT prototype that we took to the pathology lab to measure oral biopsies and correlate with histopathology. As with the Birmingham dental composite project, I wrote the control software in C#, although bits and pieces of C were used to interface with National Instruments libraries. The hardware control had to interface with various vendor DLLs. This was all on Windows. We published some of the results from this device. My interest was in linking the scattering patterns with a pathology grading of oral epithelia dysplasia. I developed a method which I called scattering attenuation microscopy. This took the 3D OCT data and represented a 2D image of the attenuation due to scattering projected onto a 2D surface view. It never caught on outside our lab, but I didn’t have the money to attend conferences to promote the idea and name as aggressively as I could have.

Instead, I received a BBSRC grant in collaboration with GSK to use my OCT measurement technology to measure the demineralisation and remineralisation process within dental enamel. My PhD student built a microfluidics system to washing solution of dental samples and quantitative imaging the time resolved changes due to the solution interaction with the enamel surface and sub-surface. Building on this work I was able to supervise multiple DDent (doctoral) student projects to explore difference solutions and some of the nuances of the technique. These project were the lifeblood of my lab which I ran as the Head of Optical Imaging Lab within the institute – a self appointed title, but useful for describing what I was doing. I had two of my own laboratories across 2 QMUL sites in which I hosted numerous (>40) student projects from engineering undergrads to dental post-grads) over the decade that I was at QMUL. I don’t remember exact dates but this was earlier in my time at QMUL, circa 2012-2014, for the GSK work, although I started the lab in 2010 and grew it all the way through to leaving at the end of 2021. It remains to this day under the auspices of Prof Graham Davis.

Whilst I enjoy building experimental optical systems, optical engineering is not my core skill. What I really enjoy doing is modelling a measurement system computationally and analysing data from that system. However, I do also enjoy building the first prototype – but refining this and optical design is best left to specialists.

During the GSK project I began to make extensive use of MatLab for processing the experimental data. It had a lot of libraries that made analysis straight forward – it was also the academic standard. I was using C# for hardware control and acquisition from devices. My code was functional, but neither pretty or maintainable – my priority has always been on getting results.

As an academic member of staff, I also had to teach/lecture. With a background in physics the fit was not immediate obvious within my clinical department. Initially I picked up undergraduate basic science lectures in cardiovascular anatomy, structure and function of cells, brain overview. I had no background in these subjects so I always had to re-learn the lectures in order to deliver them. My lectures on action potentials did become computer science lectures on artificial neural networks (circa 2010-2013), my cardiovascular anatomy lectures become fluid dynamics lectures.

I eventually transitioned to running a research methods course for all post-graduate dental students. I shaped this course to teach basic statistical methods for clinical research, i.e. study design, hypothesis testing, and basic statistical methods and analysis techniques (t-test, confidence intervals, non-parametric testing, ANOVA). I taught about research ethics, and research study types such as systematic reviews, Cochrane reviews, and the pyramid of scientific evidence. I ran hands-on tutorials on using SPSS – I ended up delivering this to PhD student groups too. The natural progression of this course was to run a research statistics clinic to support student projects. The need for this increased to the point that I was supporting a high proportion of clinical and non-clinical research projects within the institute with advice on statistical design and statistical analysis. I developed a reputation for being very pragmatic, which I hold as being necessary when helping with statistics in the real-world. This enabled me to learn an expanding number of techniques, although I can never remember them – I am confident in my ability to address any statistical need.

My post-grad statistics teaching and background in Physics found a common need in a new course, the certificate for clinical foundation studies. The course was started to serve the need of international embassies (Kuwait, UAE, Saudi) to send their sponsored students to obtain a medical or dental degree in the UK. This course served this niche market, with each student being worth upwards of £1m to the university over the course of the clinical studies. I taught some basic physics (ultrasound and medical imaging) and basic statistics on this course in it’s first year. The course was badly managed. So, in it’s second year, when applications were sought for new leadership, I applied and was appointed to run the programme as the course Director. So, I had the lofty title of “Director of Certificate for Clinical Foundation Studies”, or something like that. These days I just refer to it as Director of Foundation Medicine – most students went on to study medicine, not dentistry. My job was to bring together a disparate team and create a cohesive course with excellent student outcomes, meaning we needed a high percentage to progress to their chosen clinical degree, and a high percentage to complete their clinical training.

The primary issue was the staff team. Academia is about building your own Kingdom under your control. It really is not about working together for a common goal. The kingdom everyone is pursuing is research based, not teaching based. Teaching is something that has to be done, but for most academics it is secondary. I had a team that comprised academics from the medical school (psychologist, biologist), a former biology teacher who joined the dental school specifically to be dedicate to this programme, and a physics teacher who worked part time for the course and part time in a secondary school. I also had a chemistry teacher, who was something of an ‘alternative’ thinker who didn’t like being told what to do!

I brought this team together, developed a programme that everyone fell in love with, organised a structured programme that the students engaged with and had great success over two years of delivery. I discovered that I’m a really gifted leader, capable of bringing teams to deliver on specific goal. I’m not great at admin – but I got the support I needed to help me succeed. I hired people, managed difficult people, managed us through needing additional staff, health crises, and managed the course to completion through the COVID pandemic. The course had 20 students during my tenure of which only 1 did not complete. Since I left, the course has grown to 30 or more and become an important revenue stream for the medical and dental school.

As my academic reputation grew, I had companies reach out. Often these were small start-ups looking to access domain specific expertise – for me this was around OCT. So, I began taking on consultancy work (I did this with small companies as well as larger companies like GSK and debeers). The smaller companies preferred to work with me independently of the university, so I incorporated my company, Tomlins Analytics Ltd in 2013 to serve this need. I undertook a range of work, but most was either data analysis / data science (Python, MatLab), experimental design, clinical study design (statistics using SPSS). I also sold the NPL OCT phantoms through my company to a small number of groups worldwide (from memory this included Germany, USA, Japan). I did consultancy statistics teaching and analytics work. My turnover was low, I did this alongside my main employment. The company still exists, but I have not undertaken any consultancy work since joining Caristo in January 2021.

Through my consultancy business process, I met a number of start-up founders. I discovered that they interacted with university innovation departments to see what IP was available on their shelves, and worked out deals to spin-out the technology. This resonated with my realisation that for anyone to benefit from academic scientific research, it needs to be turned into products that people can buy. In medicine, patients will only benefit from research when someone does the hard work of bringing that research to market, overcoming the challenges of the technology, regulatory hurdles and scaling barriers. I became increasingly interested in this process to the point that this felt more meaningful than the underlying research. Although, my passion for research remains.

In my own lab, I had developed OCT technology and had students develop it piece by piece to become a clinical prototype with an intra-oral probe and medical device trolley. I had used stock optical components from Edmund Optics and Thorlabs, pre-built electronic controls that interfaced with the control computer of USB and high speed capture cards and used 3D printing to build custom mounts and the hand held probe. As a research prototype, the device could only be used for research studies. We had developed ML using Python/Keras/Tensorflow and trained models on local NVIDIA Tesla board (Quadro 4000 equivalent I think) on a basic Linux PC setup in my lab. We had developed some pretty neat de-noising techniques using autoencoders that removed the multiple scatter noise from 3D OCT images and enabled deeper light penetration within dental and oral tissue. This effectively solves the inverse scattering problem that I had become interested in during my PhD, but it achieves this statistically rather than through a direct mathematical theory. Inspired, I reached out to one of the CEOs I had met and asked for his advice on turning this research into a product. At the same time, my former Aston Uni student reached out to see if I had any thoughts on founding a company. The advice was to found a company, so in 2018 Z-Ray Imaging was incorporated. We didn’t have any money initially, so I used student projects to develop the technology in one of my labs. I then raised £50k seed investment from Barts Charity to develop the technology further. The aim was to have a handheld oral diagnostic tool capable of replacing the dental X-Ray. Lofty, but I now realise this was flawed in many ways.

With little money, and plenty of other academic work to do, Z-Ray’s progress was slow. Then, when COVID hit in 2020 we paused everything. It was during the COVID lockdown that Caristo reached out to me on LinkedIn, spotting that I might be a good fit for their open role as a Principal Research Scientist.

I was impressed by Caristo. This was innovative scientific research in cardiology form Oxford University that the researchers had managed to raise sufficient capital to turn into a medical device company. Whilst it worked with X-Ray CT (Computed Tomography Angiography), there were many parallels to my background, i.e. 3D imaging, issues of calibration, quantitative measurement of attenuation, linking to fundamental biology from imaging biomarkers, prognostic statistical modelling, ML and AI, computer vision). I was completely sold on the opportunity. So, in January 2021 I left QMUL and joined Caristo Diagnostics. I handed over Z-Ray to my two co-founders having learned a lot, but really got the venture off the ground. In 2025 it was dissolved. 
Caristo diagnostics focused predicting the risk of cardiac death (for ease of communication we often say fatal heart attacks, but it is really death from cardiac causes!) through it’s Software as a Medical Device (SaMD), delivered to clinicians using the Software as a Service (SaaS) model. Caristo’s unique predictor is that it has developed a way to measure coronary inflammation from the CCTA. The inflammation is an independent prognostic biomarker of future cardiac events. So, our prognostic risk model is incredibly powerful – it predicts risk in people who have no visible disease. Caristo has a SaaS platform, a backend written in Python, and frontend written in REACT (I used Python extensively, I do not code in REACT). The system was a 3D DICOM image viewer with segmentation tools sufficient to define the vessel centreline, label the vessels and segment them. A team of in-house image analysts (a mix of radiographers and analysts with basic science degrees) opened the CCTAs in the web-based viewer, centrelined, labelled, and segmented the relevant anatomical structures. When I joined this process was entirely manual. The segmentations were used to derive quantitative biomarkers from the CCTA which were fed into a prognostic model along with clinical risk factors to determine the risk of cardiac death. This along with a normalised measure of inflammation (FAIScore) were output onto a PDF report that was returned to the referring clinician or to their PACS as DICOM encapsulated PDF.
 
My initial role was to lead the research team as a principal scientist to fully automate this process. When I joined there were two other members of the team. My job was to lead the development of anatomical segmentation algorithms to ensure they could be delivered to our SaaS medical device product, removing some of the manual burden and enabling scalability of the service. I had my own team within Caristo but also a collaboration with Oxford University Cardiovascular Medicine, within which a research group was operated by one of our academic founders with whom I worked closely and provided support. The Oxford group also ran the ORFAN study (Oxford Risk Factors And Non Invasive Imaging Study), an observational registry of a international patients referred for CCTA as part of their routine care, along with outcomes, medication, risk factors. A subset of the study included tissue samples, and RNA sequencing. The registry collected data from over 100k patients. The largest registry of its kind worldwide. Through its license with the university, Caristo had unique access to this dataset for improvement to its models. A key task for my group was to enable automated analysis of this dataset in terms of coronary inflammation – that is, automatically analyse the coronary inflammation from all of the ORFAN CCTAs. This required not just ML tools, but my entire background in Linux, networking and IT – things I developed right at the beginning of my career. A key foundation of the Oxford University group was linking the RNA sequencing to outcomes and disease, and linking these pathways with imaging biomarkers – the emphasis was turning to radiomics, although I have encouraged the group to widen this to pure ML techniques based upon autoencoders and variational autoencoders to build latent features that capture the imaging biomarkers.

What I realised very quickly was that each team within the company were not communicating effectively. My team were not regularly talking to engineers or image analysts, so there was very little understanding of what ‘good enough’ looked like and what the requirements of a product ready ML algorithm looked like. Everyone was developing models on their own (Linux) laptops (using on-board NVIDIA GPUs), with little to no data control in place. Furthermore, researchers were not benfitting from code version control, so research code was on laptops rather than in central repositories. As a first step I encouraged the team to use GitLab for version control and commit their code to repositories. This meant that I could pull the code and we could collaborate effectively. Initially, I took the models that we had in our group for generating coronal vessel segmentations, centreline estimation and pericardium segmentation. I then combined these models into a pipeline to produce an output – bringing together the independent work of my team members. At that time this was all written in Python, the models were produced using Tensorflow along with a bunch of standard libraries such as numpy, scipy, scikit-learn, pynrrd, pydicom, requests etc. I then worked on using the backend REST API of our SaaS CCTA analysis software to upload segmentations onto CCTA cases. I did this by mimicking the activity of an image using the segmentation editing tools via the API. By doing this, analysts could inspect and correct the model output within our SaaS platform. I put in place basic measures of success to inform adoption; using Dice score as an indicator for comparison with the training and validation (i.e. comparing Dice of ML output/ corrected output with reference mean Dice in training and validation), analyst editing time (the longer an analyst spend editing or correcting an ML segmentation, then the worse it was), and comparing overall case outputs on the end-user clinical report with reference values – this was my key contribution – the quality of the ML segmentations only matters as far as it impacts the numerical output on the clinical report. I was key to designing a ‘certification’ programme for the image analysts. In this I obtained a set of reference measurements and compared individual analysts to this reference. I established a baseline level of uncertainty from existing data to determine whether analysts performance was accepted or needed further review. Such a certification system provided baseline performance for comparing automated output. I wrote the certification analysis using R markdown, outputting a well formatted report in LaTeX directly from the R markdown document using knitr. 

Certification provided baseline levels of agreement for measurements that were used as inputs to Caristo’s prognostic model. I had my team evaluate the uncertainty of the prognostic model itself using both bootstrapping and a Bayesian methods to understand the confidence intervals and credible intervals for the risk predictions. This not account for the uncertainty of measured quantities, but helped establish baseline levels of variation in the output that could be considered ‘normal’.  This further helped refine our understanding of what ‘good enough’ looks like for the ML. This was the primary activity in the first 6 months of joining and rapidly bought together the image analysts, the researchers, and the software engineers. 
With these in motion I enabled translation of the pericardium model into the SaaS product within 3 months of joining, taking a 30 minute manual task to less than 5 minutes.

At that time, Caristo’s platform only supported analysis of the three major coronary arteries. I was part of a core team along with the product lead architect and product owner who designed the functionality to extend to all coronary arteries. Together we designed a framework to enable this, making key strategic decisions for practical implementation.

By June 2021 (6 months into my time at Caristo), my direct boss (the CTO) left (also taking our DevOps engineer) Caristo to join Google. I was appointed as the acting CTO, joining the company leadership team and reporting directly to the CEO. I had three main tasks: 1) Bring the full-coronary tree analysis to product, 2) Bring automation of the coronary inflammation to product, 3) develop a new quantitative coronary plaque product – this would be fully manual in its first incarnation, but automation needed to be in view for the future. These fitted alongside my existing role, including the automated analysis of ORFAN.

So, as CTO I took on responsibility for the full engineering (product software engineering and test) and the product research function. The software development team utilised development practises derived from the agile manifesto. Although, as a team largely originating from Siemens, they had a more Germanic interpretation of ‘agile’. Indeed, I read the agile manifesto and agreed with it. I also read ‘The Pragmatic Programmer’ (quoted frequently by our architect who was anything but agile!) – I generally enjoyed the book; although believe that it encourages software engineers to take too much ownership, which is can be a problem when they believe they understand the problem a company is trying to solve better than the company leadership. But I digress, we shall return to these thoughts later! 
The role of CTO was (and remains) a perfect fit for my skillset. Our former CTO had observed that I have the unique ability to zoom out to 35000 feet and see the big picture and then zoom down into the detail. It’s a skill that I take for granted, doing this naturally. He observed that this is a rare and unique skill that made me ideal for the CTO role. In my previous role of director of the foundation studies programme at QMUL I had also developed the ability to manage challenging and difficult people. Although, managing a team of software engineers did present me with new challenges in this regard. Our architect turned out to be very difficult. Brilliant technically, but incapable of being able to make pragmatic choices to deliver on short timelines – often throwing his toys out of the pram when he felt under pressure to deliver. He was also highly critical of other’s code quality, to the point that nobody would speak up against his criticism – he set himself up as the arbiter of what was ‘good’ and ‘acceptable’ from a code quality perspective. In his mind, everything is always about code quality first. He couldn’t zoom out to see the bigger picture, that code was simply a tool to achieve a goal. I could see that whilst maintainability and scalability are critical concepts – they are also ideals that can never be achieved. Even given infinite time, a company need might completely upend all previous design choices. So, it is critical to be fast and dynamic, and know that the choices being made limit maintainability or scalability – but working with leadership to accept these limits. This is core to what I did in that role. 

Over my career I have excelled at working with difficult people by forming a strong bond of trust. Through this, I was able to gain his respect and coax him to deliver, albeit slower than I would have liked. Nevertheless, he helped me to recruit a DevOps engineer (we transitioned the role to Site Reliability Engineer – adopting the Google terminology) and undertook the DevOps/SRE function temporarily to keep things moving. Because of this, I had to rapidly learn about AWS infrastructure and devops – I learned about infrastructure as code, and what it takes to deploy a software product to the cloud from GitLab repositories. I have never cut a release, but learned enough about the moving parts to be able to make decisions and lead effectively. The architect also helped me recruit a senior software engineer who had the experience to stand his ground. I started running with this team member and developed a good relationship. Our runs become an opportunity for him to discuss what the real software needs were and make informed decisions. He was also keenly interested in what the researchers were working on, just to ensure he was aware of things that might be coming.

Anyway, returning to the quantitative plaque analysis SaMD. The brief was to develop ‘manual’ quantitative plaque analysis software that would enable us to compete with companies such as Cleerly, Heartflow, and Elucid. It would operate as a SaaS and be designed in accordance with the EU medical device regulations and ISO13485 – it also needed to gain FDA 510k clearance. A new member of the research team had expressed the desire for career development. So, I gave her the task of being ‘product manager/ product owner/ researcher’ – a bit of a mixed role, but effectively we had limited staff and I needed someone to figure our the requirements, understand how the scientific literature defined the required measurements and turn these into specifications that the software development team could implement given research prototypes. She worked with the product owner for Caristo’s main device. So, we built CaRi-Plaque from the ground up as a manual plaque analysis tool. Of course, during this design and build process the requirements changed. A CPT code became available for AI assisted plaque analysis. We needed to respond.

The plaque product depended fully on the full coronary tree analysis. Automation of the plaque product would depend upon being able to generate ground truth data manually. The requirement for AI assisted analysis needed an innovative response.

The software team could not see a way to be ‘agile’ and respond to the new requirement on a quick turnaround. However, the researchers were much more dynamic and had already been building automated segmentation tools that could help. These were now pushed to GitLab repositories as stand-alone python tensorflow/pytorch models with some basic readme and test example data. These could be pulled and run.

So, my team pulled together our available automation into a model pipeline comprising 6 ML models. I then led an initiative to work with our QA team to write a process concession to enable use of the model pipeline as preprocessing step to initialise data in the medical device. This short-term solution bought valuable time for our engineers to be able to complete the manual product release before figuring out a way to incorporate the models into the software architecture.

Just prior to the release of my plaque analysis product, our board hired a head of engineering as a dedicated lead for our software engineering and test teams (circa August 2022). I was appointed as his peer to lead the research team, my title being head of research and innovation (I have never been fully happy with this). We sat on the company leadership team reporting directly to the CEO. This was instead of appointing at CTO. Effectively splitting the role between two technical leads. I don’t think this was a great move. But, it’s where we are.

The first head of engineering was not a great company fit, failing to release the plaque software on time and compounding the ‘slowness’ issue in the engineering team, being unable to deliver fast. He was also unable to judge whether software bugs were safety issues – this is something that I am always able to determine based upon the baseline performance information that I ensured we had determined early in my employment at Caristo. It is one of my characteristics that I learn the technology that I’m working with inside out so that I am then able to make the high level judgements. Indeed, my job is more about making judgements than anything else. Needless to say, the first head of engineering was replaced in 2024 by a much more ‘can do’ head of engineering who was a much better company fit. 

I still see myself as the company CTO despite not having leadership responsibility for the engineering function. I am ultimately the person looking at what technology we should be aiming to incorporate in the future to solve either our scaling issues or consume innovations. For this work very closely with the head of engineering – helping him to manage his team to ready them for technologies that I see coming.

With the plaque device released, I was then asked by our CEO to design a regulatory study protocol for FDA 510k submission. Leaning heavily on my consultancy background and time leading the statistical advice at QMUL’s dental institute, I designed a pragmatic study. Again, I used my in depth knowledge of the literature and expected device performance to set realistic and clinically effective acceptance criteria for the study. I worked within a small team to manage the study and report the results to the FDA, joining many calls with the review team. This led to the 510k being granted in February 2025.

This was great experience, working alongside regulatory consultants and understanding how the FDA review panels operate. What I discovered is that they are more similar to my colleagues at NPL – closer to metrology and standards experts. Clinicians largely mis-understand what the FDA reviewers are asking for. My background at NPL enabled me to interpret and communicate between the two.

This was invaluable. When I took on the role of acting CTO in June 2021, Caristo was part way through a de novo submission to the FDA for the inflammation of risk prediction device. I took this role just as we received initial feedback. A number of deficiencies were raised, mainly around the lack of testing in a US population – a key requirement that was not appreciated by the original submission team (prior to my joining). I was invited to multiple meetings with the FDA and regulatory consultants that ultimately led to the withdrawal of the application and a complete redesign. The de novo project protocol design again fell to me. I designed this based upon previous feedback, setting acceptance criteria based upon the literature and analysis that was specifically asked for. This study is in review and I await FDA feedback. However, it was successfully submitted in 2025 with a well executed clinical trial thanks to my study design an statistical analysis plan.

My tools for trial and study design are R, R markdown, and Python. I used Monte Carlo methods and bootstrapping to estimate sample sizes. I also accounted for censoring issues and loss to follow-up by implementing numerical techniques in R. I do not like R and can never remember syntax. But, it is a powerful statistical language.

The success of the 510k has unlocked US and pharma revenue for plaque analysis worth upward of £5m to Caristo. This posed a challenge because our concession to use the ML pipeline outside of the medical device was due to expire in June 2025. The solution was to translate the models fully into the medical device. To achieve this, I established a controlled MLOps process to comply with the medical device regulations and upcoming EU AI Act. Using a combination of GitLab, AWS Sagemaker, and S3 my team delivered full model and data traceability and provenance, automatic documentation including validation. I collaborated with engineers to take research model inference and convert to python packages in a local pip package repository. My team orchestrated manual validation with image analysts to ensure models met their requirements for real world service delivery. This cut the release time from 6 to 3 months; possibly even better than that. The models went live and cut at least hour of plaque analysis times – although plenty more needs to be done to drive this down further.

I have moved my team away from local model training and experimentation to fully embrace AWS. I have a set of EC2 instances that people can use to code, launching AWS sagemaker jobs which read data directly from S3 buckets. This started yielding much better data management and control.

This has led to my data science platform  initiative. I have setup a data science lakehouse with and ETL pipeline that ingests data from the CaRi device deployments, transforms the data into a preprocesed state that aligns with common research use-cases and loaded this all into S3 mapped to a wide table in an RDS database. This can be queried and used to build sagemaker jobs to train models or process data all without the data ever leaving the S3 bucket ensuring that we can demonstrate compliance with GDPR and other data laws. I have found my background in Linux administration and networking is invaluable here.

At Oxford University’s AMIIC centre – our collaborators – I have helped install 4 GPU nodes each with 4xNVIDIA V100s (1 node has 4xNVIDIA A100s). I have guided their onsite IT lead to configure common NFS home directories and configure high speed storage. I help to manage this IT and data infrastructure to enable the ORFAN project. I store and provide ETL for the whole ORFAN dataset, over 100k patients with CCTA and outcomes data. The linux cluster administration falls to me. So, I have used Anisible to streamline configuration, along with a bit of bash scripting and some python. This all builds upon a background in tinkering with these technologies.

My team’s latest research has been to use LLMs to read risk factors from patient records and to build an agentic framework to perform the steps usually carried out by human image analysts. The multi-modal agents read in image and segmentation data. Then, using prompt engineering, we direct the agents to assess the images and use tools to make corrections. For initial experiments I led my team to explore langchain and crewAI to implement Agentic RAG, presenting simple examples using streamlit. Given the successful implementation of the AI framework, I have translated this into AWS bedrock, utilising knowledge bases with known image examples. The AWS managed service facilitates a scalable solution and ensure this can be deployed to our SaMD product utilising AWS’s enterprise agreements with respect to data security. The bedrock solution relies upon custom docker images running on Lambda functions to overcome some the limitations with respect to Bedrock Agents and passing images to LLMs. Consequently, our team is able to operate with cutting edge technologies ensuring that they are rapidly deployable through a controlled MLOps process.

I had led the initiative within Caristo to adopt GitHub copilot as an AI paired programmer tool. This has unlocked team productivity, accerlating innovation by enabling researchers especially to focus on their goals rather than wasting time figuring out how to understand esoteric documentation. I push the boundaries of what can be done and what is possible.

There are many other things that I am sure I have missed. However, I am now a very experienced CTO, who can see the big picture and dive into the detail – but never lost in the weeds. I rely on my ability to build a deep knowledge of the technology to make sound and practical judgements. I prioritise rapid delivery over perfections. I am not a perfectionist; I operate more at the ‘hacky’ get things done. But that does not mean I don’t care about the detail. But, I am skilled at figuring out what ‘good enough’ looks like – especially if I can quantify that statistically. This means that I can move fast when I need to, or know what the limits are.

During my career I have presented at many conferences (about 50 conference presentations) published academic journal articles, given customer presentations, board presentations, supported board members, had involvement with our investment process (although I have not yet directly been involved in investment pitches – something that I am would excel at). 

I have also examined 9 PhDs, supervised 5 PhDs, chaired conferences, organised conferences and conference session.

I am a physicist who uses software as a tool to deliver results, whether that is a product, a scientific insight, a study protocol, or a statistical analysis plan. I strongly believe that software is the tool – not the end goal.





